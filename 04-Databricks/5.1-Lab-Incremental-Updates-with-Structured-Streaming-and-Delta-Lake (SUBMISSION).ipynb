{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42af51fd-bef4-47ca-bb96-53e5e488f554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Processing Incremental Updates with Structured Streaming and Delta Lake\n",
    "In this lab you'll apply your knowledge of structured streaming and Auto Loader to implement a simple multi-hop architecture.\n",
    "\n",
    "#### 1.0. Import Shared Utilities and Data Files\n",
    "Run the following cell to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f02d4ec-6520-4906-bd69-9cd120265130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping the stream \"None\"\nStopping the stream \"None\"\nDropping the database \"dbacademy_jc7rw_virginia_edu_dewd_5_1\"\nRemoving the working directory \"dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1\"\n\nCreating the database \"dbacademy_jc7rw_virginia_edu_dewd_5_1\"\n\nPredefined Paths:\n  DA.paths.working_dir: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1\n  DA.paths.user_db:     dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db\n  DA.paths.checkpoints: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/_checkpoints\n\nPredefined tables in dbacademy_jc7rw_virginia_edu_dewd_5_1:\n  -none-\n\nSetup completed in 10 seconds\n"
     ]
    }
   ],
   "source": [
    "%run ./Includes/5.1-Lab-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431e528d-711c-4144-9ed2-5875a0de7f24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 2.0. Bronze Table: Ingest data\n",
    "This lab uses a collection of customer-related CSV data from DBFS found in */databricks-datasets/retail-org/customers/*.  Read this data using Auto Loader using its schema inference (use **`customersCheckpointPath`** to store the schema info). Stream the raw data to a Delta table called **`bronze`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176594fc-08a2-4dde-b02f-095098cabd42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1:\n",
    "# def autoload_to_table(data_source, source_format, table_name, checkpoint_directory):\n",
    "#     query = (spark.readStream\n",
    "#                   .format(\"cloudFiles\")\n",
    "#                   .option(\"cloudFiles.format\", source_format)\n",
    "#                   .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "#                   .option(\"multiLine\", \"true\")\n",
    "#                   .option(\"cloudFiles.schemaLocation\", checkpoint_directory)\n",
    "#                   .load(data_source)\n",
    "#                   .writeStream\n",
    "#                   .option(\"checkpointLocation\", checkpoint_directory)\n",
    "#                   .option(\"mergeSchema\", \"true\")\n",
    "#                   .table(table_name))\n",
    "#     return query\n",
    "\n",
    "# data_source = '/databricks-datasets/retail-org/customers/'\n",
    "# source_format = 'csv'\n",
    "# table_name = 'bronze'\n",
    "# checkpoint_directory = f'{DA.paths.checkpoints}/customers'\n",
    "# query = autoload_to_table(data_source, source_format, table_name, checkpoint_directory)\n",
    "\n",
    "data_source = '/databricks-datasets/retail-org/customers/'\n",
    "source_format = 'csv'\n",
    "checkpoint_directory = f'{DA.paths.checkpoints}/customers'\n",
    "query = (spark.readStream\n",
    "         .format('cloudFiles')\n",
    "         .option('cloudFiles.format', source_format)\n",
    "         .option('cloudFiles.schemaLocation', checkpoint_directory)\n",
    "         .load(data_source)\n",
    "         .writeStream\n",
    "         .format('delta')\n",
    "         .option('checkpointLocation', checkpoint_directory)\n",
    "         .outputMode('append')\n",
    "         .table('bronze')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e992ca75-1679-4b11-aab8-369b3bc5b7ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 2 batchs\n"
     ]
    }
   ],
   "source": [
    "# def block_until_stream_is_ready(query, min_batches=2):\n",
    "#     import time\n",
    "#     while len(query.recentProgress) < min_batches:\n",
    "#         time.sleep(5) # Give it a couple of seconds\n",
    "\n",
    "#     print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "# block_until_stream_is_ready(query)\n",
    "\n",
    "DA.block_until_stream_is_ready(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60a19272-7a96-48f1-821a-3f97df06303e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.1. Create a Streaming Temporary View\n",
    "Create a streaming temporary view into the bronze table so that we can perform transformations using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03be6ed8-4bad-46b9-bb8b-bbb6a33c2f7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark\n",
    "  .readStream\n",
    "  .table(\"bronze\")\n",
    "  .createOrReplaceTempView(\"bronze_temp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65e84cd0-c27e-4eb8-9bb5-d205b0894b17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2. Clean and Enhance the Data\n",
    "Use the CTAS syntax to define a new streaming view called **`bronze_enhanced_temp`** that does the following:\n",
    "* Skips records with a null **`postcode`** (set to zero)\n",
    "* Inserts a column called **`receipt_time`** containing a current timestamp\n",
    "* Inserts a column called **`source_file`** containing the input filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd64fdd-9b5d-45bc-b641-b4162ceacb63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- TODO 2:\n",
    "-- SELECT * FROM bronze_temp\n",
    "-- DESCRIBE EXTENDED bronze_temp -- 20 rows\n",
    "-- SHOW TABLES\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW bronze_enhanced_temp AS (\n",
    "  SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n",
    "  FROM bronze_temp\n",
    "  WHERE postcode != '0'\n",
    ")\n",
    "\n",
    "-- SELECT * FROM bronze_enhanced_temp\n",
    "-- DESCRIBE EXTENDED bronze_enhanced_temp -- 22 rows\n",
    "\n",
    "-- CREATE OR REPLACE TEMPORARY VIEW bronze_enchanced_temp AS\n",
    "-- SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n",
    "-- FROM bronze_temp\n",
    "-- WHERE postcode > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b00d377b-c499-4c5d-b72d-f3a992c8fc6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.0. Silver Table\n",
    "Stream the data from **`bronze_enhanced_temp`** to a table called **`silver`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac90bbf8-c569-48ed-aa6f-59c6de0ffa42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3:\n",
    "# table_name = 'silver'\n",
    "# checkpoint_directory = f'{DA.paths.checkpoints}/silver'\n",
    "# query = autoload_to_table(data_source, source_format, table_name, checkpoint_directory)\n",
    "\n",
    "checkpoint_directory_silver = f'{DA.paths.checkpoints}/silver'\n",
    "query_silver = (spark.table('bronze_enhanced_temp')\n",
    "         .writeStream.format('delta')\n",
    "         .option('checkpointLocation', checkpoint_directory_silver)\n",
    "         .outputMode('append')\n",
    "         .table('silver')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269439ac-61dd-49ea-ae1a-e7fe22eb14b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 2 batchs\n"
     ]
    }
   ],
   "source": [
    "DA.block_until_stream_is_ready(query_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3db953ec-fbad-49f4-b4bf-2b2e87b7f102",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3.1. Create a Streaming Temporary View\n",
    "Create another streaming temporary view for the silver table so that we can perform business-level queries using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fb7ae0-3cc4-4fdc-a39c-436888567478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark\n",
    "  .readStream\n",
    "  .table(\"silver\")\n",
    "  .createOrReplaceTempView(\"silver_temp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f627fdb-0801-466a-b551-cbb2475db2b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 4.0. Gold Table\n",
    "Use the CTAS syntax to define a new streaming view called **`customer_count_temp`** that counts customers per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae488156-8f83-4687-85c9-9ad0634d3070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- TODO 4:\n",
    "-- DESCRIBE EXTENDED silver\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW customer_count_temp AS (\n",
    "  SELECT state, COUNT(*) as customer_count\n",
    "  FROM silver_temp\n",
    "  GROUP BY state, window(current_timestamp(), \"10 minutes\", \"5 minutes\")\n",
    ")\n",
    "\n",
    "-- SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd78532-0690-4232-b7fd-6daca3c53c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>state</td><td>string</td><td>null</td></tr><tr><td>customer_count</td><td>bigint</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "state",
         "string",
         null
        ],
        [
         "customer_count",
         "bigint",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED customer_count_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fbaacc7-3332-486e-b771-8e94f312b250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-1613253574980184>\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0mwatermarkedDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"customer_count_temp\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 6\u001B[0;31m \u001B[0mwatermarkedDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithWatermark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"window\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"10 minutes\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithWatermark\u001B[0;34m(self, eventTime, delayThreshold)\u001B[0m\n",
       "\u001B[1;32m    749\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdelayThreshold\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdelayThreshold\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    750\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"delayThreshold should be provided as a string interval\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 751\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithWatermark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meventTime\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelayThreshold\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    752\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    753\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `window` cannot be resolved. Did you mean one of the following? [`customer_count_temp`.`state`, `customer_count_temp`.`customer_count`];\n",
       "'EventTimeWatermark 'window, 10 minutes\n",
       "+- SubqueryAlias customer_count_temp\n",
       "   +- View (`customer_count_temp`, [state#8733,customer_count#8734L])\n",
       "      +- Project [cast(state#7368 as string) AS state#8733, cast(customer_count#8732L as bigint) AS customer_count#8734L]\n",
       "         +- Aggregate [state#7368, window#8736], [state#7368, count(1) AS customer_count#8732L]\n",
       "            +- Filter isnotnull(current_timestamp())\n",
       "               +- Expand [[named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385], [named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]], [window#8736, customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n",
       "                  +- SubqueryAlias silver_temp\n",
       "                     +- View (`silver_temp`, [customer_id#7364,tax_id#7365,tax_code#7366,customer_name#7367,state#7368,city#7369,postcode#7370,street#7371,number#7372,unit#7373,region#7374,district#7375,lon#7376,lat#7377,ship_to_address#7378,valid_from#7379,valid_to#7380,units_purchased#7381,loyalty_segment#7382,_rescued_data#7383,receipt_time#7384,source_file#7385])\n",
       "                        +- SubqueryAlias spark_catalog.dbacademy_jc7rw_virginia_edu_dewd_5_1.silver\n",
       "                           +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5588d2e6,delta,List(),None,List(),None,Map(path -> dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver),Some(CatalogTable(\n",
       "Catalog: spark_catalog\n",
       "Database: dbacademy_jc7rw_virginia_edu_dewd_5_1\n",
       "Table: silver\n",
       "Owner: root\n",
       "Created Time: Thu Nov 09 01:00:54 UTC 2023\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.3.0\n",
       "Type: MANAGED\n",
       "Provider: delta\n",
       "Table Properties: [delta.lastCommitTimestamp=1699491653000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\n",
       "Location: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver\n",
       "Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
       "InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n",
       "Partition Provider: Catalog\n",
       "Schema: root\n",
       " |-- customer_id: string (nullable = true)\n",
       " |-- tax_id: string (nullable = true)\n",
       " |-- tax_code: string (nullable = true)\n",
       " |-- customer_name: string (nullable = true)\n",
       " |-- state: string (nullable = true)\n",
       " |-- city: string (nullable = true)\n",
       " |-- postcode: string (nullable = true)\n",
       " |-- street: string (nullable = true)\n",
       " |-- number: string (nullable = true)\n",
       " |-- unit: string (nullable = true)\n",
       " |-- region: string (nullable = true)\n",
       " |-- district: string (nullable = true)\n",
       " |-- lon: string (nullable = true)\n",
       " |-- lat: string (nullable = true)\n",
       " |-- ship_to_address: string (nullable = true)\n",
       " |-- valid_from: string (nullable = true)\n",
       " |-- valid_to: string (nullable = true)\n",
       " |-- units_purchased: string (nullable = true)\n",
       " |-- loyalty_segment: string (nullable = true)\n",
       " |-- _rescued_data: string (nullable = true)\n",
       " |-- receipt_time: timestamp (nullable = true)\n",
       " |-- source_file: string (nullable = true)\n",
       "))), tahoe, [customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1613253574980184>\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mwatermarkedDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"customer_count_temp\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mwatermarkedDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithWatermark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"window\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"10 minutes\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithWatermark\u001B[0;34m(self, eventTime, delayThreshold)\u001B[0m\n\u001B[1;32m    749\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdelayThreshold\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdelayThreshold\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    750\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"delayThreshold should be provided as a string interval\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 751\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithWatermark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meventTime\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelayThreshold\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    752\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `window` cannot be resolved. Did you mean one of the following? [`customer_count_temp`.`state`, `customer_count_temp`.`customer_count`];\n'EventTimeWatermark 'window, 10 minutes\n+- SubqueryAlias customer_count_temp\n   +- View (`customer_count_temp`, [state#8733,customer_count#8734L])\n      +- Project [cast(state#7368 as string) AS state#8733, cast(customer_count#8732L as bigint) AS customer_count#8734L]\n         +- Aggregate [state#7368, window#8736], [state#7368, count(1) AS customer_count#8732L]\n            +- Filter isnotnull(current_timestamp())\n               +- Expand [[named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385], [named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]], [window#8736, customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n                  +- SubqueryAlias silver_temp\n                     +- View (`silver_temp`, [customer_id#7364,tax_id#7365,tax_code#7366,customer_name#7367,state#7368,city#7369,postcode#7370,street#7371,number#7372,unit#7373,region#7374,district#7375,lon#7376,lat#7377,ship_to_address#7378,valid_from#7379,valid_to#7380,units_purchased#7381,loyalty_segment#7382,_rescued_data#7383,receipt_time#7384,source_file#7385])\n                        +- SubqueryAlias spark_catalog.dbacademy_jc7rw_virginia_edu_dewd_5_1.silver\n                           +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5588d2e6,delta,List(),None,List(),None,Map(path -> dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: dbacademy_jc7rw_virginia_edu_dewd_5_1\nTable: silver\nOwner: root\nCreated Time: Thu Nov 09 01:00:54 UTC 2023\nLast Access: UNKNOWN\nCreated By: Spark 3.3.0\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1699491653000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- customer_id: string (nullable = true)\n |-- tax_id: string (nullable = true)\n |-- tax_code: string (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- city: string (nullable = true)\n |-- postcode: string (nullable = true)\n |-- street: string (nullable = true)\n |-- number: string (nullable = true)\n |-- unit: string (nullable = true)\n |-- region: string (nullable = true)\n |-- district: string (nullable = true)\n |-- lon: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- ship_to_address: string (nullable = true)\n |-- valid_from: string (nullable = true)\n |-- valid_to: string (nullable = true)\n |-- units_purchased: string (nullable = true)\n |-- loyalty_segment: string (nullable = true)\n |-- _rescued_data: string (nullable = true)\n |-- receipt_time: timestamp (nullable = true)\n |-- source_file: string (nullable = true)\n))), tahoe, [customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `window` cannot be resolved. Did you mean one of the following? [`customer_count_temp`.`state`, `customer_count_temp`.`customer_count`];\n'EventTimeWatermark 'window, 10 minutes\n+- SubqueryAlias customer_count_temp\n   +- View (`customer_count_temp`, [state#8733,customer_count#8734L])\n      +- Project [cast(state#7368 as string) AS state#8733, cast(customer_count#8732L as bigint) AS customer_count#8734L]\n         +- Aggregate [state#7368, window#8736], [state#7368, count(1) AS customer_count#8732L]\n            +- Filter isnotnull(current_timestamp())\n               +- Expand [[named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385], [named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]], [window#8736, customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n                  +- SubqueryAlias silver_temp\n                     +- View (`silver_temp`, [customer_id#7364,tax_id#7365,tax_code#7366,customer_name#7367,state#7368,city#7369,postcode#7370,street#7371,number#7372,unit#7373,region#7374,district#7375,lon#7376,lat#7377,ship_to_address#7378,valid_from#7379,valid_to#7380,units_purchased#7381,loyalty_segment#7382,_rescued_data#7383,receipt_time#7384,source_file#7385])\n                        +- SubqueryAlias spark_catalog.dbacademy_jc7rw_virginia_edu_dewd_5_1.silver\n                           +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5588d2e6,delta,List(),None,List(),None,Map(path -> dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: dbacademy_jc7rw_virginia_edu_dewd_5_1\nTable: silver\nOwner: root\nCreated Time: Thu Nov 09 01:00:54 UTC 2023\nLast Access: UNKNOWN\nCreated By: Spark 3.3.0\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1699491653000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- customer_id: string (nullable = true)\n |-- tax_id: string (nullable = true)\n |-- tax_code: string (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- city: string (nullable = true)\n |-- postcode: string (nullable = true)\n |-- street: string (nullable = true)\n |-- number: string (nullable = true)\n |-- unit: string (nullable = true)\n |-- region: string (nullable = true)\n |-- district: string (nullable = true)\n |-- lon: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- ship_to_address: string (nullable = true)\n |-- valid_from: string (nullable = true)\n |-- valid_to: string (nullable = true)\n |-- units_purchased: string (nullable = true)\n |-- loyalty_segment: string (nullable = true)\n |-- _rescued_data: string (nullable = true)\n |-- receipt_time: timestamp (nullable = true)\n |-- source_file: string (nullable = true)\n))), tahoe, [customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view_name = 'customer_count_temp'\n",
    "# streamingDF = spark.sql(f\"SELECT * FROM {view_name}\")\n",
    "# watermarkedDF = streamingDF.withWatermark(\"receipt_time\", \"10 minutes\")\n",
    "\n",
    "watermarkedDF = spark.table(\"customer_count_temp\")\n",
    "watermarkedDF.withWatermark(\"window\", \"10 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7d55e8-5b80-4331-8e28-000bc98d9857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# watermarkedDF.createOrReplaceTempView(\"watermarked_customer_count_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d74549f-2c3a-4e8f-b09d-b3f9d884e1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, stream the data from the **`customer_count_by_state_temp`** view to a Delta table called **`gold_customer_count_by_state`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c5d8a0-882d-4657-a22f-6a52a4ff2f03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3909265934102951>\u001B[0m in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[0mcheckpoint_directory_gold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{DA.paths.checkpoints}/gold_customer_count_by_state'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 11\u001B[0;31m query_gold = (spark.table('customer_count_temp')\n",
       "\u001B[0m\u001B[1;32m     12\u001B[0m          \u001B[0;34m.\u001B[0m\u001B[0mwriteStream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'delta'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m          \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'checkpointLocation'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_directory_gold\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py\u001B[0m in \u001B[0;36mtable\u001B[0;34m(self, tableName)\u001B[0m\n",
       "\u001B[1;32m   1123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mStreamingQuery\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1124\u001B[0m         \u001B[0;34m\"\"\"Alias for the toTable API\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1125\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1127\u001B[0m     def toTable(\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py\u001B[0m in \u001B[0;36mtoTable\u001B[0;34m(self, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m   1200\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mqueryName\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1201\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mqueryName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mqueryName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1202\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1203\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1204\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n",
       "SubqueryAlias customer_count_temp\n",
       "+- View (`customer_count_temp`, [state#8717,customer_count#8718L])\n",
       "   +- Project [cast(state#7368 as string) AS state#8717, cast(customer_count#8716L as bigint) AS customer_count#8718L]\n",
       "      +- Aggregate [state#7368, window#8720], [state#7368, count(1) AS customer_count#8716L]\n",
       "         +- Filter isnotnull(current_timestamp())\n",
       "            +- Expand [[named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385], [named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]], [window#8720, customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n",
       "               +- SubqueryAlias silver_temp\n",
       "                  +- View (`silver_temp`, [customer_id#7364,tax_id#7365,tax_code#7366,customer_name#7367,state#7368,city#7369,postcode#7370,street#7371,number#7372,unit#7373,region#7374,district#7375,lon#7376,lat#7377,ship_to_address#7378,valid_from#7379,valid_to#7380,units_purchased#7381,loyalty_segment#7382,_rescued_data#7383,receipt_time#7384,source_file#7385])\n",
       "                     +- SubqueryAlias spark_catalog.dbacademy_jc7rw_virginia_edu_dewd_5_1.silver\n",
       "                        +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5588d2e6,delta,List(),None,List(),None,Map(path -> dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver),Some(CatalogTable(\n",
       "Catalog: spark_catalog\n",
       "Database: dbacademy_jc7rw_virginia_edu_dewd_5_1\n",
       "Table: silver\n",
       "Owner: root\n",
       "Created Time: Thu Nov 09 01:00:54 UTC 2023\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.3.0\n",
       "Type: MANAGED\n",
       "Provider: delta\n",
       "Table Properties: [delta.lastCommitTimestamp=1699491653000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\n",
       "Location: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver\n",
       "Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
       "InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n",
       "Partition Provider: Catalog\n",
       "Schema: root\n",
       " |-- customer_id: string (nullable = true)\n",
       " |-- tax_id: string (nullable = true)\n",
       " |-- tax_code: string (nullable = true)\n",
       " |-- customer_name: string (nullable = true)\n",
       " |-- state: string (nullable = true)\n",
       " |-- city: string (nullable = true)\n",
       " |-- postcode: string (nullable = true)\n",
       " |-- street: string (nullable = true)\n",
       " |-- number: string (nullable = true)\n",
       " |-- unit: string (nullable = true)\n",
       " |-- region: string (nullable = true)\n",
       " |-- district: string (nullable = true)\n",
       " |-- lon: string (nullable = true)\n",
       " |-- lat: string (nullable = true)\n",
       " |-- ship_to_address: string (nullable = true)\n",
       " |-- valid_from: string (nullable = true)\n",
       " |-- valid_to: string (nullable = true)\n",
       " |-- units_purchased: string (nullable = true)\n",
       " |-- loyalty_segment: string (nullable = true)\n",
       " |-- _rescued_data: string (nullable = true)\n",
       " |-- receipt_time: timestamp (nullable = true)\n",
       " |-- source_file: string (nullable = true)\n",
       "))), tahoe, [customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3909265934102951>\u001B[0m in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mcheckpoint_directory_gold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{DA.paths.checkpoints}/gold_customer_count_by_state'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m query_gold = (spark.table('customer_count_temp')\n\u001B[0m\u001B[1;32m     12\u001B[0m          \u001B[0;34m.\u001B[0m\u001B[0mwriteStream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'delta'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m          \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'checkpointLocation'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_directory_gold\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py\u001B[0m in \u001B[0;36mtable\u001B[0;34m(self, tableName)\u001B[0m\n\u001B[1;32m   1123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mStreamingQuery\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1124\u001B[0m         \u001B[0;34m\"\"\"Alias for the toTable API\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1125\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1127\u001B[0m     def toTable(\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py\u001B[0m in \u001B[0;36mtoTable\u001B[0;34m(self, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m   1200\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mqueryName\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1201\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mqueryName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mqueryName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1202\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1203\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1204\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nSubqueryAlias customer_count_temp\n+- View (`customer_count_temp`, [state#8717,customer_count#8718L])\n   +- Project [cast(state#7368 as string) AS state#8717, cast(customer_count#8716L as bigint) AS customer_count#8718L]\n      +- Aggregate [state#7368, window#8720], [state#7368, count(1) AS customer_count#8716L]\n         +- Filter isnotnull(current_timestamp())\n            +- Expand [[named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385], [named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]], [window#8720, customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n               +- SubqueryAlias silver_temp\n                  +- View (`silver_temp`, [customer_id#7364,tax_id#7365,tax_code#7366,customer_name#7367,state#7368,city#7369,postcode#7370,street#7371,number#7372,unit#7373,region#7374,district#7375,lon#7376,lat#7377,ship_to_address#7378,valid_from#7379,valid_to#7380,units_purchased#7381,loyalty_segment#7382,_rescued_data#7383,receipt_time#7384,source_file#7385])\n                     +- SubqueryAlias spark_catalog.dbacademy_jc7rw_virginia_edu_dewd_5_1.silver\n                        +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5588d2e6,delta,List(),None,List(),None,Map(path -> dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: dbacademy_jc7rw_virginia_edu_dewd_5_1\nTable: silver\nOwner: root\nCreated Time: Thu Nov 09 01:00:54 UTC 2023\nLast Access: UNKNOWN\nCreated By: Spark 3.3.0\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1699491653000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- customer_id: string (nullable = true)\n |-- tax_id: string (nullable = true)\n |-- tax_code: string (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- city: string (nullable = true)\n |-- postcode: string (nullable = true)\n |-- street: string (nullable = true)\n |-- number: string (nullable = true)\n |-- unit: string (nullable = true)\n |-- region: string (nullable = true)\n |-- district: string (nullable = true)\n |-- lon: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- ship_to_address: string (nullable = true)\n |-- valid_from: string (nullable = true)\n |-- valid_to: string (nullable = true)\n |-- units_purchased: string (nullable = true)\n |-- loyalty_segment: string (nullable = true)\n |-- _rescued_data: string (nullable = true)\n |-- receipt_time: timestamp (nullable = true)\n |-- source_file: string (nullable = true)\n))), tahoe, [customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nSubqueryAlias customer_count_temp\n+- View (`customer_count_temp`, [state#8717,customer_count#8718L])\n   +- Project [cast(state#7368 as string) AS state#8717, cast(customer_count#8716L as bigint) AS customer_count#8718L]\n      +- Aggregate [state#7368, window#8720], [state#7368, count(1) AS customer_count#8716L]\n         +- Filter isnotnull(current_timestamp())\n            +- Expand [[named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 0) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385], [named_struct(start, precisetimestampconversion(((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - (((precisetimestampconversion(current_timestamp(), TimestampType, LongType) - 0) + 300000000) % 300000000)) - 300000000) + 600000000), LongType, TimestampType)), customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]], [window#8720, customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n               +- SubqueryAlias silver_temp\n                  +- View (`silver_temp`, [customer_id#7364,tax_id#7365,tax_code#7366,customer_name#7367,state#7368,city#7369,postcode#7370,street#7371,number#7372,unit#7373,region#7374,district#7375,lon#7376,lat#7377,ship_to_address#7378,valid_from#7379,valid_to#7380,units_purchased#7381,loyalty_segment#7382,_rescued_data#7383,receipt_time#7384,source_file#7385])\n                     +- SubqueryAlias spark_catalog.dbacademy_jc7rw_virginia_edu_dewd_5_1.silver\n                        +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5588d2e6,delta,List(),None,List(),None,Map(path -> dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver),Some(CatalogTable(\nCatalog: spark_catalog\nDatabase: dbacademy_jc7rw_virginia_edu_dewd_5_1\nTable: silver\nOwner: root\nCreated Time: Thu Nov 09 01:00:54 UTC 2023\nLast Access: UNKNOWN\nCreated By: Spark 3.3.0\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1699491653000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nLocation: dbfs:/user/jc7rw@virginia.edu/dbacademy/dewd/5.1/5_1.db/silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- customer_id: string (nullable = true)\n |-- tax_id: string (nullable = true)\n |-- tax_code: string (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- city: string (nullable = true)\n |-- postcode: string (nullable = true)\n |-- street: string (nullable = true)\n |-- number: string (nullable = true)\n |-- unit: string (nullable = true)\n |-- region: string (nullable = true)\n |-- district: string (nullable = true)\n |-- lon: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- ship_to_address: string (nullable = true)\n |-- valid_from: string (nullable = true)\n |-- valid_to: string (nullable = true)\n |-- units_purchased: string (nullable = true)\n |-- loyalty_segment: string (nullable = true)\n |-- _rescued_data: string (nullable = true)\n |-- receipt_time: timestamp (nullable = true)\n |-- source_file: string (nullable = true)\n))), tahoe, [customer_id#7364, tax_id#7365, tax_code#7366, customer_name#7367, state#7368, city#7369, postcode#7370, street#7371, number#7372, unit#7373, region#7374, district#7375, lon#7376, lat#7377, ship_to_address#7378, valid_from#7379, valid_to#7380, units_purchased#7381, loyalty_segment#7382, _rescued_data#7383, receipt_time#7384, source_file#7385]\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 5:\n",
    "# checkpoint_directory_silver = f'{DA.paths.checkpoints}/silver'\n",
    "# query_silver = (spark.table('bronze_enhanced_temp')\n",
    "#          .writeStream.format('delta')\n",
    "#          .option('checkpointLocation', checkpoint_directory_silver)\n",
    "#          .outputMode('append')\n",
    "#          .table('silver')\n",
    "#         )\n",
    "\n",
    "checkpoint_directory_gold = f'{DA.paths.checkpoints}/gold_customer_count_by_state'\n",
    "query_gold = (spark.table('customer_count_temp')\n",
    "         .writeStream.format('delta')\n",
    "         .option('checkpointLocation', checkpoint_directory_gold)\n",
    "         .outputMode('append')\n",
    "         .table('gold_customer_count_by_state')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853b65f6-afa4-43a8-b014-3d11115b02a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.block_until_stream_is_ready(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca186f2a-62ff-4b1f-be75-936190dbd058",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.0. Query the Results\n",
    "Query the **`gold_customer_count_by_state`** table (this will not be a streaming query). Plot the results as a bar graph and also using the map plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12a846b2-8a76-4c20-accc-8a350befa08f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM gold_customer_count_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90cbf4de-2269-4a8a-9d23-be8128621149",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6.0. Clean Up\n",
    "Run the following cell to remove the database and all data associated with this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2633ccc7-cde5-4fd1-8351-268d39b8d97e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27599803-99c9-411d-bb91-1f12d0c8a214",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "By completing this lab, you should now feel comfortable:\n",
    "* Using PySpark to configure Auto Loader for incremental data ingestion\n",
    "* Using Spark SQL to aggregate streaming data\n",
    "* Streaming data to a Delta table"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1613253574980185,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5.1-Lab-Incremental-Updates-with-Structured-Streaming-and-Delta-Lake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
